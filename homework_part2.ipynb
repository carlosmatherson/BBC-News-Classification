{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":1872300,"sourceType":"datasetVersion","datasetId":1114664}],"dockerImageVersionId":30918,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Limitation(s) of sklearnâ€™s NMF Library \n\nDTSA 5510 - Unsupervised Algorithms in Machine Learning\n\nUniversity of Colorado Boulder","metadata":{}},{"cell_type":"markdown","source":"## Part 1\n### Import Modules","metadata":{"_uuid":"b34f9e1c-f8d5-46bd-b740-6babc2041f69","_cell_guid":"8f2214e6-933e-4f65-9650-ec9b8455747d","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom collections import namedtuple\nfrom scipy.sparse import csr_matrix\nfrom sklearn.decomposition import NMF\nimport time","metadata":{"_uuid":"2fd07f7c-e487-4498-b5ba-ee94b1b663b1","_cell_guid":"62a021de-a9bd-4802-a0c3-18d4c43593af","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-03-21T04:27:36.362595Z","iopub.execute_input":"2025-03-21T04:27:36.363227Z","iopub.status.idle":"2025-03-21T04:27:36.368647Z","shell.execute_reply.started":"2025-03-21T04:27:36.363180Z","shell.execute_reply":"2025-03-21T04:27:36.367421Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Load Data\n\nLoading the same MovieLens 1M dataset from the homework. Luckily, it was hosted here on kaggle.","metadata":{"_uuid":"21de88a3-5b94-43fe-bb51-7804a1fba632","_cell_guid":"83d42397-923a-49fd-a32a-9a69124d03e2","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"users_df = pd.read_csv('/kaggle/input/movielens-1m-dataset/users.dat',\n                       header=None, \n                       sep='::', \n                       names=['UserID','Gender','Age','Occupation','Zip-code'], \n                       engine='python',\n                       encoding='latin-1')\n\n\nmovies_df = pd.read_csv('/kaggle/input/movielens-1m-dataset/movies.dat',\n                        header=None,\n                        sep='::',\n                        names=['MovieID', 'Title', 'Genre'], \n                        engine='python',\n                        encoding='latin-1')\n\nratings_df = pd.read_csv('/kaggle/input/movielens-1m-dataset/ratings.dat',\n                         header=None,\n                         sep='::',\n                         names=['UserID','MovieID','Rating','Timestamp'], \n                         engine='python',\n                         encoding='latin-1')","metadata":{"_uuid":"7ab9ad2f-01c3-4dcd-9f78-87c54f257f91","_cell_guid":"d27d1597-2165-4c37-854a-483796769f54","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-03-21T04:42:08.598085Z","iopub.execute_input":"2025-03-21T04:42:08.598547Z","iopub.status.idle":"2025-03-21T04:42:14.084654Z","shell.execute_reply.started":"2025-03-21T04:42:08.598513Z","shell.execute_reply":"2025-03-21T04:42:14.083714Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Below, I split the ratings data into a train subset (80%) and a test subset (20%). Then, I put it all together into a names tuple for convenience and print the first few rows of each data subset.","metadata":{}},{"cell_type":"code","source":"train_df, test_df = train_test_split(ratings_df, test_size=0.2, random_state=19)\n\nData = namedtuple('Data', ['users','movies','train','test'])\ndata = Data(users_df, movies_df, train_df, test_df)\n\nprint('\\n',data.users.head())\nprint('\\n',data.movies.head())\nprint('\\n',data.train.head())\nprint('\\n',data.test.head())","metadata":{"_uuid":"97102643-a363-41d8-ab03-0021937cd69f","_cell_guid":"1776f238-d340-4035-8dac-a99401a301c0","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-03-21T04:42:14.085763Z","iopub.execute_input":"2025-03-21T04:42:14.086019Z","iopub.status.idle":"2025-03-21T04:42:14.174435Z","shell.execute_reply.started":"2025-03-21T04:42:14.085997Z","shell.execute_reply":"2025-03-21T04:42:14.173544Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### NMF Matrix Factorization Custom Implementation\n\nInitialized like the RecSys class from the original homework notebook. Has methods for building the rating matrix, fitting NMF, making predictions, and scoreing using RMSE.","metadata":{"_uuid":"42930302-6a5a-4c21-82c7-93967287df6d","_cell_guid":"9b9452c6-4e36-445b-88de-059241457a8f","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"class MatrixFactorization():\n    \n    def __init__(self, data):\n            self.data=data\n            self.allusers = list(self.data.users['UserID'])\n            self.allmovies = list(self.data.movies['MovieID'])\n            self.mid2idx = dict(zip(self.data.movies.MovieID,list(range(len(self.data.movies)))))\n            self.uid2idx = dict(zip(self.data.users.UserID,list(range(len(self.data.users)))))\n            self.Mr=self.rating_matrix()\n            self.W = None  # User factors\n            self.H = None  # Item factors\n            self.reconstructed = None  # Reconstructed matrix\n            self.n_components_ = None\n\n    def rating_matrix(self):\n\n        ind_movie = [self.mid2idx[x] for x in self.data.train.MovieID] \n        ind_user = [self.uid2idx[x] for x in self.data.train.UserID]\n        rating_train = list(self.data.train.Rating)\n        \n        return csr_matrix((rating_train, (ind_user, ind_movie)), shape=(len(self.allusers), len(self.allmovies)))\n\n    \n    def fit_nmf(self, n_components=1, max_iter=200, beta_loss='frobenius', init='random', solver='cd'):\n        \n        Mr_dense = self.Mr.toarray()\n        \n        global_mean = Mr_dense[Mr_dense > 0].mean()\n        \n        # impute with global mean \n        Mr_filled = Mr_dense.copy()\n        Mr_filled[Mr_filled == 0] = global_mean\n        \n        # initialize and fit NMF model            \n        model = NMF(n_components=n_components, max_iter=max_iter, beta_loss=beta_loss, \n                    init=init, solver=solver,random_state=19)\n        \n        start_time = time.time()\n        self.W = model.fit_transform(Mr_filled)\n        self.H = model.components_\n        self.n_components_ = model.n_components_\n        \n        # rebuild the ratings matrix\n        self.reconstructed = np.dot(self.W, self.H)\n        \n        elapsed = time.time() - start_time\n        print(f\"NMF fitting time {elapsed}\")\n        \n        return self\n    \n    def predict_from_nmf(self, uid, mid):\n\n        if self.reconstructed is None:\n            return 3.0  \n        \n        user_idx = self.uid2idx[uid]\n        movie_idx = self.mid2idx[mid]\n        \n        # clip to range (1-5)\n        prediction = self.reconstructed[user_idx, movie_idx]\n        return np.clip(prediction, 1, 5)\n    \n    def predict(self):\n\n        uids = self.data.test['UserID'].values\n        mids = self.data.test['MovieID'].values\n        \n        predictions = np.array([self.predict_from_nmf(uid, mid) for uid, mid in zip(uids, mids)])\n        \n        return predictions\n        \n    def rmse(self,yp):\n        yp[np.isnan(yp)]=3 #In case there is nan values in prediction, it will impute to 3.\n        yt=np.array(self.data.test.Rating)\n        return np.sqrt(((yt-yp)**2).mean())","metadata":{"_uuid":"8c562eef-b442-430c-80ab-81cacb0b1602","_cell_guid":"cc76352e-e48f-4088-90d6-d5d8640e8ca7","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-03-21T04:48:03.437149Z","iopub.execute_input":"2025-03-21T04:48:03.437594Z","iopub.status.idle":"2025-03-21T04:48:03.461258Z","shell.execute_reply.started":"2025-03-21T04:48:03.437560Z","shell.execute_reply":"2025-03-21T04:48:03.460021Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":54},{"cell_type":"code","source":"for k in [1, 50, 100, 150]:\n\n        mf = MatrixFactorization(data)\n\n        mf.fit_nmf(n_components=k)\n\n        predictions = mf.predict()\n        rmse = mf.rmse(predictions)\n\n        print(f\"NMF with {mf.n_components_} components RMSE: {rmse}\")","metadata":{"_uuid":"6c55327b-eb63-4c08-b77e-005ad758e438","_cell_guid":"e176eb69-c8b1-47f4-aec4-4f3042845c81","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-03-21T04:48:05.289857Z","iopub.execute_input":"2025-03-21T04:48:05.290193Z","iopub.status.idle":"2025-03-21T04:51:43.118410Z","shell.execute_reply.started":"2025-03-21T04:48:05.290166Z","shell.execute_reply":"2025-03-21T04:51:43.117130Z"},"jupyter":{"outputs_hidden":false}},"outputs":[{"name":"stdout","text":"NMF fitting time 0.3909461498260498\nNMF with 1 components RMSE: 1.1737815382990575\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/sklearn/decomposition/_nmf.py:1665: ConvergenceWarning: Maximum number of iterations 200 reached. Increase it to improve convergence.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"NMF fitting time 28.286161184310913\nNMF with 50 components RMSE: 1.0726620197231886\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/sklearn/decomposition/_nmf.py:1665: ConvergenceWarning: Maximum number of iterations 200 reached. Increase it to improve convergence.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"NMF fitting time 62.16630959510803\nNMF with 100 components RMSE: 1.0983095865459571\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/sklearn/decomposition/_nmf.py:1665: ConvergenceWarning: Maximum number of iterations 200 reached. Increase it to improve convergence.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"NMF fitting time 117.34989380836487\nNMF with 150 components RMSE: 1.119522350478055\n","output_type":"stream"}],"execution_count":55},{"cell_type":"markdown","source":"## Part 2\n\nThe NMF approach yielded high RMSE values. The best RMSE achieved was 1.07, which is within the range of values achieved with similarity based methods, but not particularly impressive. The table below shows each method used and their RMSE.\n\n|Method|RMSE|\n|:----|:--------:|\n|Baseline, $Y_p$=3|1.26 |\n|Baseline, $Y_p=\\mu_u$|1.14 |\n|Content based, item-item|1.19 |\n|Collaborative, cosine|1.14 |\n|Collaborative, jaccard, $M_r\\geq 3$|0.98  |\n|Collaborative, jaccard, $M_r\\geq 1$|0.99  |\n|Collaborative, jaccard, $M_r$|0.96|\n\nIt is clear that the matrix factorization approach (RMSE 1.07) is much better than the baseline methods that imputed values with either the median rating (RMSE 1.26) or the mean rating (RMSE 1.14). Non-negative Matrix Factorization is also superior to the content based item-item (RMSE 1.19) and collaborative appraoch with cosine similarity (RMSE 1.14) methods. \n\nNMF falls short compared to collaboritve recommendation using jaccard similarity in general. Three strategies were used to construct the ratings (utility) matrix for the collaborative recommendation methods. The first was to use only the good ratings (>2 on a 1-5 scale), and this achieved an RMSE of 0.98. The second was to use only the existing ratings (>0), which achieved an RMSE of 0.99. The third was to use the data without applying a transformation to achieve the lowest RMSE of 0.96.\n\nThe inferior performance can likely be attributed to NMF's inability to handle sparse data. The other recommendation methods can take advantage of sparse matrices, but NMF requires dense matrices and imputation. Also, the better-performing jaccard-based methods transform contuous ratings into binary, which may allow this approach to capture preference patterns more accurately than the continuous ratings used by NMF and the rest.\n\nOverall, NMF is too generic to compete with the more specialized recommendation systems. To improve the error, it may be beneficial to transform the utlity matrix to contain binary values as seen in the jaccard-based methods. Additionally, optimizing the hyperparameters can likely improve the capacity to recognize preference patterns.\n","metadata":{"_uuid":"6f606639-6932-4a34-93ec-5574b4a9e8a8","_cell_guid":"a087eb4b-24d8-43b6-a1c7-8f94e32340af","collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}